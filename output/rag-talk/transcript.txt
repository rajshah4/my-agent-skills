[00:00:00] Are you ready for a deep dive into rag,
[00:00:02] especially around retrieval? What I want
[00:00:04] to do today is talk to you about h what
[00:00:07] the problems are with rag, how you scope
[00:00:10] out rag use cases, but most of what
[00:00:12] we're going to focus on is a technical
[00:00:14] deep dive around some specific retrieval
[00:00:16] approaches, BM25, language models, and
[00:00:19] then finally, agentic search and rag.
[00:00:22] This is a presentation I gave recently
[00:00:24] at a conference at the MLOps conference.
[00:00:27] What I'm going to do today is going to
[00:00:28] go a little bit faster through the
[00:00:30] content. You're going to miss some of
[00:00:31] the jokes, some of the setups like that,
[00:00:34] but I want to give everybody a chance to
[00:00:36] understand some of the content that I
[00:00:38] was able to share out there. Now, to
[00:00:40] start out with, I like to focus on where
[00:00:43] we are with Rag today. And lots of folks
[00:00:46] are out there kind of building their own
[00:00:48] RAG solutions because after all, we see
[00:00:50] chat GBT, you're inside a company, you
[00:00:53] want to be able to ask questions, right?
[00:00:55] No, the CEO wants to know, hey, can I
[00:00:57] get the list of board of directors from
[00:00:59] your fancy chat GBT solution? And you
[00:01:02] quickly find out, no, it doesn't because
[00:01:04] you need your enterprise knowledge in
[00:01:07] there. All of you probably at this point
[00:01:10] know, right? You can build a very easy
[00:01:12] rag demo out of the box by just grabbing
[00:01:15] some data, using an embedding model,
[00:01:17] creating vectors, doing the similarity.
[00:01:19] The code is out there. Many of us have
[00:01:22] kind of run through code like this
[00:01:23] before where we take our information, we
[00:01:26] chunk it down, we can then retrieve it,
[00:01:28] pass it to a language model. I'm going
[00:01:30] to assume everybody here kind of knows
[00:01:32] the basics for kind of building rag. And
[00:01:35] it's easy to build a rag solution like
[00:01:37] this. But where I come in and hope why
[00:01:40] you're probably here is you know that
[00:01:42] the reality is you can build that quick
[00:01:45] demo but as you try to move that into
[00:01:48] production you figure out that the
[00:01:51] accuracy of what you did isn't that
[00:01:53] great because maybe you focused on some
[00:01:55] using some synthetic data to generate to
[00:01:58] evaluate your model that was based
[00:02:00] around simple extraction but your users
[00:02:02] ask much more complex type of questions.
[00:02:05] So your accuracy drops as soon as you
[00:02:07] move it into production. Or it could be
[00:02:10] you had so many calls that the latency
[00:02:12] is just unbearable and your users don't
[00:02:14] want to wait that long. It could be just
[00:02:17] a problem of scaling, right? Going from
[00:02:20] a 100 documents to a,000, 10,000, a
[00:02:23] million documents. Lots of issues around
[00:02:26] scaling there that happen. Finally, it's
[00:02:30] the cost, right? Your demo works great,
[00:02:32] but we're seeing a lot of push back in
[00:02:34] enterprises as you built a very complex
[00:02:36] multi-agent rag system that all of a
[00:02:38] sudden each query costs like a dollar
[00:02:41] for each search. It's like it's crazy.
[00:02:44] And then finally, and this is the thing
[00:02:46] most people miss, but inside an
[00:02:47] enterprise, not everybody gets to read
[00:02:49] every document. You need to have some
[00:02:52] way to work with compliance, some method
[00:02:54] for entitlements. So, not everybody can
[00:02:57] see every document like that. And I know
[00:03:00] I say this and many of you will scour
[00:03:02] and look at your algorithms and be like,
[00:03:05] well, you know what, Raj, I'm just going
[00:03:06] to go search out on archive and there's
[00:03:09] got to be a better paper out there. And
[00:03:11] there's lots of papers out there on rag
[00:03:15] like that. But what I want to do is the
[00:03:17] answer is not in here of pulling
[00:03:20] together like a bunch of archive papers
[00:03:23] and doing this. So I want to give people
[00:03:24] a framework for thinking through how to
[00:03:27] solve the rag problems and not ending up
[00:03:29] with something that looks like this.
[00:03:33] So as a starting point, let me give you
[00:03:35] the idea first of all is you have to
[00:03:37] think of rag as a system. It's not just
[00:03:39] one or two models. You're going to have
[00:03:41] a lot of different parts together as you
[00:03:44] build a rag production system. And that
[00:03:46] involves parsing the documents cleanly
[00:03:49] getting it out. something that's vastly
[00:03:51] overlooked, but especially when you get
[00:03:53] to complex documents, you need a good
[00:03:55] system for getting that information out
[00:03:57] cleanly. How you query your documents,
[00:04:00] the retrieval, finding those chunks, and
[00:04:03] then generating a response. So, this out
[00:04:07] here is kind of a baseline system for
[00:04:09] today. What I want to first do is talk
[00:04:12] about how we figure out where to improve
[00:04:15] and what type of system we need.
[00:04:18] And this is where like whenever somebody
[00:04:20] comes to me with a rag problem before we
[00:04:22] jump into the technical details, I like
[00:04:25] to know what are the trade where are
[00:04:27] they thinking in terms of tradeoffs.
[00:04:29] Right? This is just like like another
[00:04:31] engineering problem where you have to
[00:04:33] think about how quickly you want
[00:04:35] something. What's the cost going to be?
[00:04:38] And instead of accuracy here, I'm using
[00:04:40] problem complexity.
[00:04:42] And if you can't have that serious
[00:04:43] discussion with your users and know what
[00:04:45] the trade-offs are, it's going to be
[00:04:47] hard to actually build a useful
[00:04:49] solution. So have that upfront
[00:04:51] conversation. Set your expectations of
[00:04:54] users. One thing that often comes up in
[00:04:57] Gen AI um is the cost of a mistake.
[00:05:01] Often where you're going to get
[00:05:02] implemented in today's generative AI
[00:05:04] because they are mistakrone is in places
[00:05:08] where it's easy to make mistakes. So
[00:05:10] this is why you see codebased tools,
[00:05:12] codebased assistant tools happening all
[00:05:14] the time because developers can quickly
[00:05:15] check the mistake while building a rag
[00:05:18] system that might be a life and death
[00:05:20] something for patients in a healthcare
[00:05:22] medical might be a little bit harder
[00:05:24] because of the cost of the mistakes. But
[00:05:26] that should be a rough proxy for telling
[00:05:28] you like how what you're going to have
[00:05:30] to do to get this into production.
[00:05:33] Now, to give you a sense of kind of some
[00:05:34] of the trade-offs that we have, this is
[00:05:37] something I made I made this like
[00:05:39] probably a year ago for our sales team.
[00:05:41] Like my day job is helping to kind of
[00:05:43] work with customers on building kind of
[00:05:45] rag solutions. I've been doing this for
[00:05:47] years all the way back to when I was at
[00:05:49] HuggingFace. And so I built this for the
[00:05:51] sales guy. This this chart on the right
[00:05:53] just I use chatbt to build this. But the
[00:05:56] idea here is to have that conversation
[00:05:59] about what type of queries, what type of
[00:06:01] things people are going to use this rag
[00:06:03] system for. And that way you can figure
[00:06:06] out what is the complexity, what are the
[00:06:07] nuances that you're going to have to
[00:06:09] consider. Understanding this upfront
[00:06:12] before you start getting into the
[00:06:13] details of of designing your rag system
[00:06:15] is very important and could save you a
[00:06:18] lot of heartache. Like one thing I like
[00:06:20] to do is just know what kind of queries
[00:06:22] do we expect from people because as we
[00:06:25] talk about queries, they have a lot of
[00:06:27] different complexity where you could
[00:06:29] have very simple keyword queries, right?
[00:06:31] This will work very great. But on the
[00:06:34] other hand, maybe your users don't know
[00:06:36] the exact keywords and you're going to
[00:06:38] have some type of semantic variation
[00:06:40] where they're going to say how much bank
[00:06:42] instead of just the total revenue.
[00:06:45] Or maybe your users have queries where
[00:06:47] they expect a little bit of complexity
[00:06:49] of hopping from get solving one part of
[00:06:51] the question going to the next part. Or
[00:06:54] maybe the questions answers are in
[00:06:56] multiple documents.
[00:06:58] Or and this is the one that often
[00:07:00] happens a lot of times the answers
[00:07:02] aren't in the documents that they give
[00:07:04] you that they're all of a sudden they're
[00:07:06] asking for knowledge that's outside. I
[00:07:08] see this happening all the time and it's
[00:07:10] something again to get ahead of to
[00:07:12] figure out how you want to solve it. And
[00:07:14] then finally nowadays we are used to
[00:07:17] these thinking LLMs. People are asking
[00:07:19] much more broader questions that call
[00:07:22] for agentic scenarios. Each of these
[00:07:25] things will come with consequences for
[00:07:27] how we design our systems like that.
[00:07:32] Now today I'm going to focus on the
[00:07:36] retrieval part of this diagram. And I do
[00:07:38] this because this is where I see a lot
[00:07:41] of confusion, uncertainty. I people
[00:07:44] don't know exactly what type of models
[00:07:45] to pick out. They're a little bit
[00:07:47] overwhelmed at all the different
[00:07:48] architectures and different choices. And
[00:07:51] I see a lot of stuff out on the
[00:07:53] internets, on the LinkedIn, the X's
[00:07:55] where there's a lot of different
[00:07:56] philosophies, developers, companies
[00:07:58] putting out kind of ideas like that. And
[00:08:01] I want to give people a good solid
[00:08:03] practical way to start with retrieval
[00:08:06] before jumping into all types of
[00:08:08] alternative approaches like that. So
[00:08:10] this is where we're going to focus on
[00:08:12] retrieval. We're going to stick to the
[00:08:13] basics here. And retrieval again is
[00:08:16] about I have a document. I've broken it
[00:08:19] down into pieces, right? We need to
[00:08:21] break these things down into pieces.
[00:08:23] You're not going to have one model that
[00:08:25] can hold them all. If you just think
[00:08:27] about the extreme case, imagine you had
[00:08:29] one model that had like a 10 million
[00:08:31] context length that could hold
[00:08:33] everything. Would it really make sense
[00:08:35] to run that for every query? the amount
[00:08:39] of compute to do that. No, no, no. We
[00:08:42] want to be compute efficient. It makes
[00:08:43] sense to break our documents into pieces
[00:08:46] using a chunking approach like that.
[00:08:48] This is why retrieval rag is always
[00:08:50] going to be around. We're going to talk
[00:08:52] about BM25.
[00:08:54] We're going to talk about language
[00:08:55] models and agentic search. And since
[00:08:57] this is a video where you can skip
[00:08:58] ahead, I'm going to just keep going.
[00:09:01] Now, if you think about it in terms of
[00:09:03] code, this is kind of the area of the
[00:09:05] steps that we're working in where we've
[00:09:08] chunked down the data. The next step is
[00:09:10] figuring out what the relevant chunk is
[00:09:12] to pass into our generator model. A lot
[00:09:16] of approaches here. I want to start with
[00:09:18] BM25.
[00:09:20] BM25
[00:09:22] stands for best match 25. So, it's a
[00:09:25] lexical type of search.
[00:09:27] And it took a couple iterations for them
[00:09:29] to get there. This is the 25th, right?
[00:09:31] This is a crafted formula that takes a
[00:09:34] look at all the words that are in a
[00:09:36] document and creates this inverted
[00:09:38] index. And so you can see it right here
[00:09:40] in the middle where like for every word
[00:09:42] we know which documents it goes to. So
[00:09:45] then right if I have a search and I need
[00:09:47] to f figure out I have a query that has
[00:09:49] about butterfly now I can figure out
[00:09:51] what is the relevant document that has
[00:09:53] butterfly right like this is a very nice
[00:09:56] way of doing this and to give you a
[00:09:59] sense of like how quick and how fast it
[00:10:01] is I kind of use chatbt to get a quick
[00:10:05] benchmark so all of these numbers were
[00:10:07] just generated using the models which is
[00:10:09] why I don't have a ton of code for you
[00:10:11] because you can easily get the models
[00:10:13] like this so I had to create a synthetic
[00:10:15] set of a,000 3,000 9,000 documents.
[00:10:19] The first thing I did was I ran a linear
[00:10:22] search. So a linear search is just like
[00:10:24] when you use Ctrl+F find and you look
[00:10:26] for one after another after another
[00:10:28] after another. So imagine looking for
[00:10:30] that word butterfly right through each
[00:10:32] document one time after time. And you
[00:10:34] can see right that scales as you import
[00:10:37] documents. It's going to scale and go
[00:10:39] much slower. And it's pretty slow. As
[00:10:41] soon as you get to right a thousand
[00:10:43] documents, you can see it already takes
[00:10:44] 3,000 seconds to do that. On the other
[00:10:48] hand, if we use this idea of created
[00:10:50] this inverted index
[00:10:53] and then use the BM25 algorithm which
[00:10:55] works off that inverted index, you can
[00:10:58] see it's much much faster, right?
[00:10:59] Several orders of speed, much faster,
[00:11:02] which is why this is a widely used
[00:11:05] approach like that.
[00:11:08] So, what's the rub? Why aren't we using
[00:11:10] this all the time? Well, this is great
[00:11:12] at keywords, but here's the thing is
[00:11:16] sometimes
[00:11:17] the queries, the things we look for
[00:11:20] don't exactly match up in a lexical
[00:11:23] one-on-one overlapping way.
[00:11:26] Somebody asks, in this case, for a
[00:11:29] physician, but all the documents have
[00:11:32] the word doctor or somebody asked for
[00:11:34] international business machines, but the
[00:11:37] documents use IBM. So these are the
[00:11:39] failure words of this kind of strict
[00:11:42] lexical approach. Now nevertheless this
[00:11:46] is a very strong baseline. In fact when
[00:11:48] we talk about these neural network
[00:11:50] methods many of these methods the BM25
[00:11:53] can beat on lots of different types of
[00:11:55] corpus. So depending on the type of
[00:11:58] documents and the type of queries you
[00:11:59] have this can be a good fit. Can give
[00:12:02] you a pretty good baseline like that.
[00:12:04] especially if you have users that
[00:12:05] already kind of know the words that
[00:12:07] they're looking for uh like that. And as
[00:12:09] we'll talk about later in a gentic
[00:12:11] search, it might be sufficient for a lot
[00:12:12] of cases. For those of you who want to
[00:12:14] play around with this, there's an
[00:12:16] implementation of BM25 in Python that
[00:12:19] you can go and grab and play around
[00:12:21] with.
[00:12:24] Now I want to enter language models. So
[00:12:27] the idea of language models is what we
[00:12:30] can do is we can take our text that we
[00:12:32] have, we pass it through an encoder,
[00:12:34] right? It's an encoder because it's
[00:12:36] encoding all of our text into into into
[00:12:39] numbers. And with that encoding, then
[00:12:43] we're going to be able to use that as a
[00:12:44] way to search. And the great way is
[00:12:47] because the language models are trained
[00:12:48] on lots and lots of data, they've seen a
[00:12:50] lot of the world, they have an idea of
[00:12:52] these similar concepts. So when for
[00:12:55] example I search for physician
[00:12:58] what I can do is in that same latenc
[00:13:03] number that we've created from the
[00:13:04] embedding the word doctor is very close
[00:13:07] by right international business machines
[00:13:10] is very close to IBM this is basic
[00:13:12] semantic search stuff it works really
[00:13:15] well which is why it was a big deal for
[00:13:17] Google when they added it they had great
[00:13:20] results with it. It's also very popular.
[00:13:22] If you go over to hugging face, you'll
[00:13:24] see these models are widely deployed. So
[00:13:26] this is all kind of um baseline
[00:13:28] knowledge that you should have. Now the
[00:13:31] question though a lot of people have is
[00:13:32] like okay Raj you told me this but like
[00:13:36] which one of these should I use.
[00:13:38] So now let's look at the data here. If
[00:13:41] we look at this graph we'll see there's
[00:13:42] two axes. One is CPU inference speed and
[00:13:46] this is just basically how fast it goes
[00:13:48] right faster to the right slower to the
[00:13:50] left and could have been GPU. I just
[00:13:53] used CPU implementation here for this
[00:13:56] this what is this NCDG thing all this so
[00:13:58] NCDG is a retrieval metric it's the
[00:14:00] quality of how good the results come
[00:14:02] back nano is a small version of BEI B
[00:14:07] beer BIR which is a benchmark for
[00:14:09] information retrieval so this is
[00:14:11] basically a quality score on a benchmark
[00:14:14] for testing out information retrieval so
[00:14:17] the higher you get the better your job
[00:14:20] you're doing at retrieval so I've given
[00:14:22] you the big map. So, I started helping
[00:14:25] you here by putting where BM25 is,
[00:14:28] right? You can see it's over to the
[00:14:29] right, right? It's one of the faster
[00:14:31] approaches as we talked about, but right
[00:14:34] in terms of retrieval quality, it's not
[00:14:36] necessarily the best. But look,
[00:14:40] is it is it the fastest?
[00:14:43] No. There's even a faster way that I
[00:14:47] want to show you, which is using static
[00:14:49] embeddings. What a static embedding is
[00:14:52] is is for every word is going to have a
[00:14:55] unique num numerical embedding. So this
[00:14:58] is very similar to older approaches like
[00:15:00] word tovec that you might have seen. So
[00:15:03] here for example the word happy the word
[00:15:05] joy has a kind of a meaning. You can run
[00:15:09] this really quickly. It's super fast.
[00:15:11] You can even run this on kind of CPUs.
[00:15:14] Now the downside of kind of using the
[00:15:16] static embeddings is many words have
[00:15:19] multiple meanings right like you have to
[00:15:22] look at the context to understand that
[00:15:25] and so this approach is going to lose
[00:15:28] out in some of those cases where that
[00:15:31] context matters for understanding the
[00:15:33] meaning here right so model for example
[00:15:35] can mean right a statistical model that
[00:15:38] most of us are used to in kind of AI and
[00:15:40] data science but if you're in fashion
[00:15:43] Right? Model has a very different
[00:15:45] meaning like that. So this is where the
[00:15:48] context matters and where our retrieval
[00:15:51] quality can suffer is because people ask
[00:15:54] questions um that have different ways.
[00:15:59] And this is where there's a lot more
[00:16:01] models that are available way over here
[00:16:03] to the left. And you can see some of the
[00:16:04] names out here. And they differ of
[00:16:06] course in retrieval quality. They differ
[00:16:10] in speed. Right? If you think about
[00:16:12] this, and we'll talk about why is this
[00:16:14] one way over here that's super super
[00:16:16] super slow. Well, and it's a bigger
[00:16:19] model takes a lot more compute to run.
[00:16:23] So now you're going to ask me, all
[00:16:26] right, so you got me a ton more models
[00:16:28] here. How am I going to figure out kind
[00:16:31] of which model to run? Well, the folks
[00:16:35] over at HuggingSpace hosts a couple of
[00:16:37] leaderboards. So this is the multi-ext
[00:16:40] embedding leaderboard. And now the
[00:16:42] retrieval embedding leaderboard that has
[00:16:44] hundreds of models out there. And I want
[00:16:46] to take a minute here and show you. I'm
[00:16:48] going to flip over on my web browser.
[00:16:50] This is what the front page of the
[00:16:53] leaderboard looks like. So here, for
[00:16:55] example, I'm looking at the multi-
[00:16:57] lingual part of the leaderboard. Um,
[00:17:00] you'll see there's many many models
[00:17:02] here. I think on the there's the order
[00:17:04] of like 300 models. And the great thing
[00:17:08] is is is not only do you get the basic
[00:17:10] characteristics what are the embedding
[00:17:12] dimensions what are the tokens of that
[00:17:16] but you get all this look over to the
[00:17:17] right you get all these scores here as
[00:17:20] well. So these are how this model
[00:17:23] performs on some public data sets that
[00:17:27] are out there and it gives us a rough
[00:17:29] gauge for being able to compare
[00:17:31] different models. And so this is where
[00:17:33] you can now pick the model that best
[00:17:35] kind of fits your use case. And as you
[00:17:38] would kind of explore this, you'll see
[00:17:39] that there's many different kinds of
[00:17:42] ways to kind of break up these these
[00:17:44] embedding models and look and find the
[00:17:47] right one. Recently, what what's
[00:17:49] happened is is we've they've introduced
[00:17:51] a new one here, the the retrieval
[00:17:54] embedding leaderboard. And I'm going to
[00:17:56] click on this one here.
[00:17:58] And with the retrieval embedding
[00:18:00] leaderboard,
[00:18:04] there it goes. The retrieval embedding
[00:18:05] leaderboard's a little bit different and
[00:18:08] it's going to have a subset. It doesn't
[00:18:10] have all the models yet, but it has one
[00:18:13] important difference. And the multi-ext
[00:18:17] embedding leaderboard all showed public
[00:18:19] data sets performance. What the
[00:18:22] retrieval embedding leaderboard is going
[00:18:24] to show is it shows performance on a
[00:18:27] private held out data set. And this is
[00:18:30] important because this keeps us from
[00:18:32] keeps people from kind of gaming the
[00:18:34] system and maybe training on the exact
[00:18:36] same data that's there. Unfortunately,
[00:18:38] this thing is taking forever to load
[00:18:41] today. Um, like that. So, let me stop
[00:18:44] and wait for it to load.
[00:18:51] And there you can see that you can see
[00:18:52] the private scores right here for that
[00:18:55] leaderboard as well. Now there's a ton
[00:18:58] of tools here that you can use to help
[00:19:01] kind of look at the leaderboards to
[00:19:03] figure out kind of what's important like
[00:19:05] that.
[00:19:07] One of the things that often comes into
[00:19:09] mind for as you're picking these is the
[00:19:11] size of the model of the size of the
[00:19:14] model because the size of the model will
[00:19:16] directly have to do with compute. Now,
[00:19:19] when I when I do this in in in a
[00:19:22] classroom setting or a workshop, I have
[00:19:23] a little more fun. But what I want to
[00:19:25] point out here is take a look here.
[00:19:27] You'll see that there's a bunch of
[00:19:29] models here that are all the same size,
[00:19:32] right? So, it takes the same amount of
[00:19:34] compute to run these models, but the
[00:19:37] performance over here on the mean
[00:19:39] differs.
[00:19:40] And the reason why and I'll give you one
[00:19:42] second if you want to pause and come up
[00:19:44] with your own explanation is the models
[00:19:46] have improved for the same size models
[00:19:49] people have evolved better training
[00:19:51] strategies for example improved
[00:19:53] architectures and so we have kind of a
[00:19:56] newer generation of models where we have
[00:19:58] a slight improvement um like that. So
[00:20:01] this is one of the great things we'll
[00:20:02] talk about here is these things are
[00:20:04] moving along as well.
[00:20:08] Now, as you start thinking about them,
[00:20:10] there's of course the accuracy of the
[00:20:12] models, how well they perform on the
[00:20:14] task, the latency and compute. All of
[00:20:16] these things are tied together.
[00:20:19] You'll want to think about, hey, is this
[00:20:20] an open source model? Can I go and
[00:20:22] quantize it maybe to get a little bit of
[00:20:24] extra speed? I'll think about the
[00:20:26] embedding dimension. Is this a small
[00:20:28] kind of embedding of 128 or is this kind
[00:20:30] of a gargantuan one? Because I'm going
[00:20:33] to have to manage and store all that
[00:20:34] stuff as well. When you're starting to
[00:20:37] look deeper into these models, you can
[00:20:39] look at, for example, some of them have
[00:20:40] trained differently, multilingual, for
[00:20:43] example, or maybe there's a specific
[00:20:44] domain. Maybe you're going to go and
[00:20:46] fine-tune the model. A lot of kind of
[00:20:48] different considerations that come into
[00:20:50] place like that.
[00:20:53] One kind of neat kind of innovation that
[00:20:56] I want to highlight here is these the
[00:20:58] matriosia embedding models where a lot
[00:21:02] of embedding models initially came with
[00:21:04] one dimension. So when you passed in
[00:21:06] your information out came one token one
[00:21:09] embedding um output of a fixed length.
[00:21:14] What the match should let us do is let
[00:21:16] us pick we can pick smaller lengths like
[00:21:20] that. So this can be more convenient for
[00:21:22] storage or for compute that we're doing
[00:21:24] but the models have been trained. So
[00:21:25] even if you go down to let's say 64 the
[00:21:28] retrieval quality is still high. I
[00:21:31] believe the open AI models also support
[00:21:33] this capability as well.
[00:21:36] So it's kind of an interesting thing I
[00:21:38] like to kind of highlight for folks um
[00:21:40] with these models. Now as you start
[00:21:42] using these models, one of the most
[00:21:44] dominant models you will see are
[00:21:46] sentence transformers for retrieval
[00:21:48] tasks. And these work great because if
[00:21:51] you think about most of the documents
[00:21:52] you write you you read that you're using
[00:21:55] for retrieval, they're probably all like
[00:21:57] written in sentences. Well, the sentence
[00:21:59] transformer has been drained to work
[00:22:01] with documents, not at like the
[00:22:03] individual token level, but in terms of
[00:22:06] sentences. So, it works a lot better
[00:22:08] kind of for retrieval. And you'll see
[00:22:10] these widely used inside of RAG systems
[00:22:14] like that.
[00:22:16] A second model you'll see is the cross
[00:22:19] encoder or often as we use it in rag, we
[00:22:22] often call it a reranker because that's
[00:22:24] the function it has inside there. And
[00:22:26] what the cross encoder does is it
[00:22:28] crosses two things. It crosses the
[00:22:30] incoming embedding that we have with the
[00:22:35] query that's coming in. So maybe your
[00:22:37] retriever found a bunch of chunks. We
[00:22:39] cross that chunk with the query. And
[00:22:41] what that does is you can use that cross
[00:22:43] encoder to get a score to see how
[00:22:45] similar they are. And what we do is
[00:22:48] we'll then use that to check each one of
[00:22:50] those embeddings. And that will often
[00:22:53] lead to a reranking stage where we find
[00:22:55] out the relationship of that query is a
[00:22:57] little bit better to some chunks. And
[00:23:00] what we can do is we can get a slight
[00:23:02] improvement in performance like that. So
[00:23:04] this is widely used um in that. And
[00:23:07] you'll see you typically get a little
[00:23:09] bit of a bump by using a reranker but
[00:23:11] like everything right it doesn't come
[00:23:13] for free. There's often kind of some
[00:23:15] type of latency gain. And again the
[00:23:18] numbers here I'm just to give you the
[00:23:19] ideas. You can of course pick re-rankers
[00:23:22] of different um that of different sizes
[00:23:24] and different kind of latency effects
[00:23:26] like that.
[00:23:28] Now I have a an example kind of collab
[00:23:31] demo that I taken from somebody else but
[00:23:34] it's a nice walkthrough to kind of give
[00:23:36] you a sense of like what the value is of
[00:23:38] a reranker how to kind of use one just
[00:23:41] all runs inside Google collab um for
[00:23:43] those who want to kind of play around
[00:23:44] with the code a little bit. Couple of
[00:23:47] other things. I work at Contextual. We
[00:23:48] launched an instruction following
[00:23:50] reranker. It is out there on hugging
[00:23:52] face. You can use it for free if you're
[00:23:54] in a company um that's for research, but
[00:23:57] if you want to actually use it, you got
[00:23:58] to pay for it. Um but the key thing here
[00:24:02] is it allows you to send an instruction
[00:24:06] or a prompt for how you want that
[00:24:08] ranking to happen. So you can prioritize
[00:24:11] different types of information. This
[00:24:13] gives you another kind of knob when
[00:24:15] you're thinking about these reranking
[00:24:16] models. Now, a lot of retrievers, I
[00:24:20] showed you a lot of methods before.
[00:24:22] Well, you know what? You don't have to
[00:24:23] use them by themselves. You can combine
[00:24:25] them. So, here, for example, you can see
[00:24:27] here some combinations of maybe I use
[00:24:29] BM25 with something else. Maybe I used
[00:24:32] E5 with the re-ranker. Or maybe, right,
[00:24:34] like you go crazy and I'm like, shove
[00:24:36] them. You put them all together, right?
[00:24:38] They did a fusion here and re-rank all
[00:24:40] the way, right?
[00:24:42] You can do that, but just remember,
[00:24:44] right, like you got to engineer this.
[00:24:46] You're going to have to pay for the
[00:24:47] compute. You're going to have to keep
[00:24:48] track of everything if you want to use
[00:24:50] multiples. But you have a lot of
[00:24:52] flexibility when it comes to these
[00:24:53] retrievers depending on your needs,
[00:24:55] right? Like if you look over at the
[00:24:56] folks at Kaggle, they'll do some crazy
[00:24:58] stuff like this. In a recent Kaggle
[00:25:00] competition, this person used three
[00:25:03] different rerankers to get to the final
[00:25:05] kind of list where they went from 64 to
[00:25:08] eight to five to rank the top five like
[00:25:10] that. So depending on what you need,
[00:25:13] there's a lot you can do against it. But
[00:25:16] my best practice always is
[00:25:19] use a hybrid search. So semantic and
[00:25:21] lexical,
[00:25:23] fuse those together, right? Reciprocal
[00:25:25] rank fusions easily out there. Pass it
[00:25:27] through a reranker. That's going to give
[00:25:29] you pretty good standard performance out
[00:25:32] of the box. And this is a great place as
[00:25:34] a baseline to set before you kind of
[00:25:37] jump into lots of alternative approaches
[00:25:40] um like that.
[00:25:42] So a lot of different families of models
[00:25:45] tried to cover all these. If not kind of
[00:25:47] send your questions and comments like
[00:25:49] that. And this is an area of course with
[00:25:50] embedding models where you see kind of
[00:25:53] newer embedding models coming out just
[00:25:55] in the last couple weeks. The folks over
[00:25:57] at John Hopkins, IBM, Google all
[00:25:59] released models. I would tell you though
[00:26:01] as a practitioner it there is a slight
[00:26:04] improvement enough where you want to use
[00:26:05] the latest models but like it's not a
[00:26:07] gamecher. I wouldn't like go rip up
[00:26:09] something you have to install kind of a
[00:26:11] newer model as well. So the improvements
[00:26:14] are very incremental at this point. Now
[00:26:18] there's a lot of other retrieval methods
[00:26:20] like I'm only taking like maybe an hour
[00:26:22] of time. We could easily do a six-hour
[00:26:24] workshop on everything in rag. So,
[00:26:27] splade for example is a very popular way
[00:26:29] for kind of sparse um sparse retrieval
[00:26:33] methods where what you do is you add a
[00:26:35] few other kind of related synonyms
[00:26:37] inside your inside your lexical search
[00:26:40] area at as a way to improve it. There's
[00:26:43] other things like coar which is named
[00:26:45] after the late night talk show host late
[00:26:47] interaction approach. There's graph rag
[00:26:50] where you spend a lot of time upfront
[00:26:52] like creating this whole graph network
[00:26:54] and all the entities relationships like
[00:26:57] there were some funny kind of tweets on
[00:26:59] X about like the fact that everybody
[00:27:01] talks about graph rag but nobody
[00:27:03] actually implements it or uses it.
[00:27:05] There's tons of other rag flavors out
[00:27:07] there. Will they work for your use case?
[00:27:10] Absolutely they may. But again like I'm
[00:27:13] giving you a good baseline. Use that as
[00:27:15] a starting point to decide like hey
[00:27:16] where are we lacking? Where are we
[00:27:18] failing? Will one of these methods kind
[00:27:20] of fit where the gaps are that we have
[00:27:22] like that before just start before just
[00:27:24] start following kind of the flavor of
[00:27:26] the week that you see out there. In
[00:27:29] terms of operational concerns, a couple
[00:27:31] of things I want to mention is there are
[00:27:33] optimized libraries for doing some of
[00:27:34] this stuff. So face for example from
[00:27:37] Facebook is widely used for kind of
[00:27:39] computing embeddings.
[00:27:42] The other thing is depending on the
[00:27:43] amount of embeddings you have, the
[00:27:45] amount of documents, you might be able
[00:27:47] to just store them in memory. For a lot
[00:27:49] of use cases, you can just buy a big
[00:27:50] processor, store them in memory, load
[00:27:52] them up, run what you need to like that.
[00:27:55] On the other hand, other use cases,
[00:27:58] you're like, I want a database. I want
[00:27:59] someplace I can manage that, be able to
[00:28:02] keep updates and do all of that. Sure,
[00:28:04] absolutely. And there's a ton of vector
[00:28:06] database options and I do not pretend to
[00:28:09] tell you kind of where to go in terms of
[00:28:11] that. I've linked one of the an article
[00:28:13] that I found in here. A big part of when
[00:28:15] you pick out vector databases is
[00:28:17] thinking about what is your latency
[00:28:19] requirements for that, right? Because
[00:28:22] that's going to govern the the kind of a
[00:28:24] choice of it. But a lot of the standard
[00:28:26] databases out there, the snowflakes of
[00:28:28] the world for example, now give you some
[00:28:30] way to kind of store your embeddings and
[00:28:31] vectors there as well.
[00:28:36] All right. Um,
[00:28:38] one other thing is is as you kind of
[00:28:41] when we talk about these retrieval
[00:28:42] methods, what we typically find is that
[00:28:44] as your information grows,
[00:28:48] your those retrieval methods, you're
[00:28:50] going to have to lean on much heavier
[00:28:52] for you. And this is one of the
[00:28:54] techniques I like to recommend to people
[00:28:56] to use as metadata. when you go from a
[00:28:59] thousand documents to a million
[00:29:00] documents, if you're not using something
[00:29:02] like metadata, so when you have a
[00:29:04] search, you can narrow down the scope,
[00:29:07] it's going to be very tough to be able
[00:29:09] to find and get your high accuracy like
[00:29:11] that. So yes, retrieval will go down as
[00:29:14] your data source goes up, but you can
[00:29:16] use strategies to help fight that back.
[00:29:20] All right, so now let's move into the
[00:29:22] exciting part. We've talked about
[00:29:24] traditional rag kind of oneshot
[00:29:26] approaches to be able to search. There's
[00:29:29] also now agentic rag where we use
[00:29:32] multiple ways multiple kind of calls to
[00:29:35] be able to do this. This all stemmed in
[00:29:39] just basically the last year really came
[00:29:42] to a head because now we have LM
[00:29:45] reasoning models that are capable of
[00:29:48] effectively using tools where they can
[00:29:51] use a tool to make a query look at the
[00:29:54] results that come back and go oh you
[00:29:56] know what maybe we should do it a
[00:29:58] different way like that. So that's been
[00:30:00] the radical change which to me has
[00:30:02] really brought this about. So I have a
[00:30:04] code snippet here. Agno for example is
[00:30:06] an open source library. They have
[00:30:08] examples of kind of reasoning agents
[00:30:11] like that. Um the basic kind of approach
[00:30:14] here is your query comes in, you're
[00:30:17] going to generate an answer and then you
[00:30:20] ask your LM did it answer the question
[00:30:22] or not? If not, it can rewrite the
[00:30:24] query, word it a little bit different,
[00:30:26] try to find that missing information,
[00:30:28] feed that back into the loop as well.
[00:30:32] And what ends up looking like something
[00:30:34] like this where it your model's thinking
[00:30:37] through the steps of what it needs to be
[00:30:39] able to do for each of those steps it
[00:30:42] will be like oh this is the query I need
[00:30:45] to make this is what this is the search
[00:30:46] information I need to based on those
[00:30:49] results it'll come back and that gives
[00:30:51] you to a nice response that takes a
[00:30:54] little bit longer but now is much more
[00:30:57] kind of searched across to be able to do
[00:31:00] this
[00:31:02] one of The big early categories of this
[00:31:03] was deep research. So the folks over at
[00:31:06] Langchain have an open source repo with
[00:31:08] their open deep research approach which
[00:31:11] again takes a similar approach. I mean
[00:31:13] they do some things like use a bunch of
[00:31:15] research sub agents to go out there and
[00:31:17] be able to kind of do get that do those
[00:31:20] retrievalss and then bring it all back
[00:31:22] together to write a report. You can
[00:31:24] check it out. They have they've made all
[00:31:26] the prompts all that stuff available. So
[00:31:28] it's good to see out. There's a whole
[00:31:30] school around this like actually doing
[00:31:32] this deep research where there's deep
[00:31:34] research bench which is a leaderboard in
[00:31:36] this space. Um it can get very expensive
[00:31:40] if you start running a lot of a lot of
[00:31:42] tasks on it. It's got a 100 PhD level
[00:31:44] research tasks. So it can cost a little
[00:31:46] bit of money for each of the queries to
[00:31:49] run. But if you're interested in this
[00:31:51] kind of deep research area and I already
[00:31:53] see companies out there um making out
[00:31:56] making solutions for this. I think the
[00:31:58] interesting thing on this one here is
[00:32:00] you pick the time that you want to think
[00:32:02] and that's often as we'll talk about
[00:32:04] here a crucial element when it comes to
[00:32:07] using this agentic search approaches.
[00:32:11] A widely used benchmark that I find
[00:32:13] interesting in this area is called
[00:32:15] bright and it's a benchmark that's built
[00:32:18] around retrieval reasoning. So you can
[00:32:21] see at the top here there's two examples
[00:32:22] of keyword and semantic. This isn't a
[00:32:25] benchmark that does that. There's other
[00:32:27] ones that does that. Instead, look at
[00:32:29] the examples that it's giving you. These
[00:32:31] are the types of questions that it wants
[00:32:34] to do where you have to do a little bit
[00:32:36] deeper kind of thinking through things
[00:32:38] to be able to solve the problems like
[00:32:41] this. So, it's an excellent benchmark
[00:32:43] out there. If we look and see what is
[00:32:46] the number one approach right now on
[00:32:48] this, its name is diver. And if you take
[00:32:52] a look at the diver diagram,
[00:32:54] it probably doesn't look that crazy to
[00:32:56] you if you're used to rag like right you
[00:32:58] see, oh wait a minute, right? There's a
[00:32:59] there's a chunking mechanism here.
[00:33:01] There's a retrieving mechanism here. Oh,
[00:33:04] there's a re-ranking mechanism here. And
[00:33:06] in fact, the number one I also looked at
[00:33:08] the number two approaches both use those
[00:33:10] same type of methodologies that we
[00:33:12] talked about for a baseline in this.
[00:33:14] Now, they've tweaked this, of course,
[00:33:16] around kind of the the reasoning pieces,
[00:33:18] but again, the elements that we've
[00:33:20] talked about of going out, finding
[00:33:22] information, chunking, finding
[00:33:24] information, re-ranking, but then just
[00:33:26] putting this into a loop is what's going
[00:33:28] on here. Um, and you can see here some
[00:33:31] of the examples of queries where, right,
[00:33:34] in the first round they have, hey, given
[00:33:35] a query, go look for this stuff. And
[00:33:37] now, right, based on what we've learned,
[00:33:40] now what do you think would be possibly
[00:33:42] helpful to do like that? And so this is
[00:33:45] hopefully gives you the schematic of how
[00:33:46] we can think about kind of a gentic rag
[00:33:49] where we're going to ask, hey, can we
[00:33:52] find a better answer? And look, there's
[00:33:54] been people doing this. This isn't brand
[00:33:56] new. There's things like self-rag where
[00:33:58] people have asked and had the models
[00:34:00] kind of reflect on the answers like
[00:34:02] that. I saw this on Reddit where
[00:34:04] somebody kind of shared out um hey how
[00:34:07] they built this system to do that. But
[00:34:11] one of the things you quickly find out
[00:34:12] is that these systems can be super
[00:34:15] inefficient. Right? They're asking the
[00:34:17] models to go back and retrieve rer
[00:34:21] and maybe it could have been done just a
[00:34:23] lot quicker. So this is right there's
[00:34:26] always a rub. This is the rub with these
[00:34:28] approaches is that long latency and the
[00:34:31] long agentic time. And in fact, this is
[00:34:34] uh kind of I ran this myself, right? I
[00:34:36] work at Contextual. We have kind of a
[00:34:38] RAD platform, lots of little settings.
[00:34:40] So I could kind of tweak it and play
[00:34:42] around with this. And I took Wix QA,
[00:34:45] which is publicly available. It's on
[00:34:47] HuggingFace. It's a technical support um
[00:34:51] technical support database uh technical
[00:34:53] support knowledge base. So that's the
[00:34:55] kind of question it has like a help desk
[00:34:57] questions and you can see when I had my
[00:34:59] kind of the one one one shot rag and I
[00:35:02] think this is like E5 with maybe a
[00:35:04] reanker I can't remember
[00:35:07] I could get answers really quick right 5
[00:35:09] seconds or so it boom it pulls it back
[00:35:13] but in terms of factuality which is the
[00:35:15] benchmark they use on Wix QA like it's
[00:35:18] 76 like yeah it's pretty good but but
[00:35:21] it's missing a lot well then I wired in
[00:35:24] diagentic rag system where all I did was
[00:35:27] just asked it to reflect you know maybe
[00:35:28] do more retrieval calls if the answer
[00:35:30] wasn't complete
[00:35:32] and it would do more retrieval calls and
[00:35:35] you can see here it took a little bit
[00:35:36] longer in terms of thinking the numbers
[00:35:39] here don't fixate on the exact numbers
[00:35:41] there's a lot of wiggle room I just want
[00:35:43] to give you a sense of like a gentic rag
[00:35:45] takes a little bit longer to do that but
[00:35:47] on the other hand take a look at that
[00:35:49] accuracy that's crazy that's a look at
[00:35:52] that improvement there that has a ton of
[00:35:55] implications for that. I mean, one thing
[00:35:58] right away for me when I ran this is
[00:36:00] look now now anything that I'm missing
[00:36:03] that I'm not getting in this 007. I kind
[00:36:05] of wonder like even my smart system that
[00:36:07] took a while couldn't figure it out.
[00:36:09] Like what is going on with the last
[00:36:11] little bit here? But beyond that, look
[00:36:13] at the difference between the 0.93 and
[00:36:15] 76. So these were answers that I was
[00:36:18] able to figure out if I did multiple
[00:36:20] calls. On the other hand, right, these
[00:36:23] are all the the bottom ones here were
[00:36:24] all the ones I did the first time. So,
[00:36:26] there's a gap here that of answers that
[00:36:28] took a couple of calls to do. Well, why
[00:36:31] don't I take a look at those? Why don't
[00:36:33] I take a look at those types of queries,
[00:36:36] that type of material, and figure out
[00:36:38] why do they take multiple queries? Is
[00:36:41] there something about the way my
[00:36:42] documents are stored? Is there a way of
[00:36:44] how I've chunked the documents retrieval
[00:36:46] that I can modify so I could actually
[00:36:49] improve this oneshot rag and bring it up
[00:36:52] closer? Right? You can think of this now
[00:36:54] as like this is our new kind of baseline
[00:36:56] for what's possible. Right? This shows
[00:36:58] what's possible in the system. I can now
[00:37:01] up that up. So this really is is
[00:37:03] mind-blowing for me in a couple of
[00:37:05] different ways. Um like that.
[00:37:08] Now it's I'm not going to stop you
[00:37:10] there. So one of the things in the
[00:37:12] bright paper is they had this graph and
[00:37:15] I want to walk you through it. So over
[00:37:17] here they have Quen. So this is the Quen
[00:37:19] embedding models. So they're using the
[00:37:22] Quen embedding models with GPT4. So
[00:37:26] doing that same kind of retrieval loop
[00:37:28] that we've talked about to be able to
[00:37:30] find kind of answers. And you can see
[00:37:32] here's the quality. The higher the bar
[00:37:34] you get the better answer. So you can
[00:37:36] see right the Quinn model does okay.
[00:37:38] Right? It's much better than this espert
[00:37:40] model bottle over here or right the
[00:37:42] instant model does. But it gives you a
[00:37:44] sense of like the quality of what we're
[00:37:46] getting. But wait a minute, what's this
[00:37:49] other crazy model over here? This BM25.
[00:37:53] Take a look. It does better than the
[00:37:55] Quen. This is crazy, right? Like our
[00:37:58] good old keyword baseline.
[00:38:02] There's just the language model because
[00:38:03] it can rewrite the queries, right? like
[00:38:06] it doesn't need an embedding model to
[00:38:07] figure out the semantic variation
[00:38:09] because it can just rewrite that into
[00:38:11] different semantic ones and re-query
[00:38:13] re-query like that and it actually does
[00:38:16] better than the quen embedding model
[00:38:18] like I saw this in the graph and
[00:38:20] immediately I was like research team
[00:38:21] like like this is this is something we
[00:38:24] need to dive into and I even ran my own
[00:38:26] benchmarks like this on a couple of the
[00:38:28] data sets we have the Wix QA and then I
[00:38:30] we have an internal financial data set
[00:38:32] of 10ks and you know what like When I
[00:38:35] ran this with just BM25,
[00:38:38] it still did a lot better than one shot
[00:38:40] and it still got pretty close to kind of
[00:38:43] the Aentic rag. And this is all just out
[00:38:45] of the box. I've not tuned or tried to
[00:38:47] make this improve this thing in any way.
[00:38:49] This is just the raw performance. But
[00:38:52] this blows me away, right? Like now this
[00:38:55] tells me I don't need to use that neural
[00:38:59] network model, the the semantic model,
[00:39:01] all that stuff, those embeddings, vector
[00:39:04] databases. is I could throw all that
[00:39:05] away. I could just stick this in a
[00:39:08] textonly database and use BM25 and get a
[00:39:11] do a pretty good job of being able to
[00:39:13] find the right answers consistently,
[00:39:15] right? These are in across different
[00:39:16] domains. So for me, this is a real
[00:39:18] gamecher.
[00:39:20] Now there is the downside, right?
[00:39:22] There's that latency cost, right? Like
[00:39:24] you're lots of times you're going to
[00:39:25] need one shot. People are going to need
[00:39:27] an answer in three or five seconds or
[00:39:28] they're going to leave your website or
[00:39:30] they're going to be unhappy. There's
[00:39:32] plenty of room, many use cases for
[00:39:35] singleshot rag. It's not going away in
[00:39:37] any way like that. But, you know, if the
[00:39:40] accuracy matters, if people can wait,
[00:39:42] this aentic rag is really going to
[00:39:44] change things. And you already see it
[00:39:46] being used in terms of like things like
[00:39:48] clawed code for example uses a lexical
[00:39:51] approach, an iterative approach for its
[00:39:54] um for its approach. And part of that is
[00:39:57] it's doing code search, which is a
[00:39:58] little bit different. code doesn't have
[00:40:00] the same kind of semantic properties
[00:40:02] that conventional documents do. So, it's
[00:40:05] always been a little weird for using it
[00:40:07] for search, but still it's another sign
[00:40:09] that, you know, hey, maybe we can use
[00:40:11] these agentic approaches um to be able
[00:40:13] to do stuff.
[00:40:15] Now, as we're doing this, I like to keep
[00:40:19] you all practically minded. This is
[00:40:20] where we talk about tradeoffs, but you
[00:40:22] know what? These things can work
[00:40:23] together, too. So, I saw this um paper
[00:40:27] by Door Dash where they talked about
[00:40:29] their guardrail system. And what I liked
[00:40:31] is it showed a two-tier system where it
[00:40:33] showed two different methods working
[00:40:35] together. It uses a very simple text
[00:40:38] similarity
[00:40:40] as an initial guardrail. If that
[00:40:43] guardrail works great, that's great. If
[00:40:46] not, they can always kick it over to an
[00:40:48] LLM, but they use the LM as a backup,
[00:40:50] right? It's not the primary. It's it's
[00:40:52] more of like a difficult cases. And I
[00:40:55] like this type of thinking because it
[00:40:57] better takes advantage of the relevant
[00:41:00] kind of advances ad trade-offs of these
[00:41:03] different types of approaches. Um
[00:41:05] there's an here's an example if you want
[00:41:07] to play around with agents. There's lots
[00:41:09] of agent um ones out there, but just
[00:41:11] wanted to give people um a little sense
[00:41:14] of that. All right. Thank you all. I
[00:41:18] going through this. Hopefully, I've
[00:41:19] given you some sense of like thinking
[00:41:21] about these trade-offs, you know, right
[00:41:23] based on kind of these trade-offs. Maybe
[00:41:25] if you have like a high cost of mistakes
[00:41:27] or really, right, you don't want to make
[00:41:28] mistakes. You have the budget for it,
[00:41:31] maybe a reranker could make sense. On
[00:41:33] the other hand, right, maybe maybe
[00:41:34] you're like, "Hey, Raj, you know, I need
[00:41:36] sub 5 seconds, sub 1 second latency."
[00:41:39] So, maybe you're like, "Hey, I'm going
[00:41:40] to try out that BM25 or static
[00:41:42] embeddings." Maybe your users are doing
[00:41:44] complex multihop queries. So, a gentic
[00:41:47] rag is a better better fit like that.
[00:41:50] Um, but hopefully I've gone through
[00:41:52] given you a little bit of a cert
[00:41:54] checklist here for kind of good starting
[00:41:57] points for kind of building out your rag
[00:42:00] systems like that. I'm going to end it
[00:42:02] here. When I do this in person, I ask a
[00:42:05] lot of questions. We walk through other
[00:42:07] parts of the rag platform as well. But
[00:42:10] thank you all.