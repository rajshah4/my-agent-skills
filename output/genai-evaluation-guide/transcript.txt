[00:00:00] evaluation for your generative AI
[00:00:02] applications. This was a recent talk I
[00:00:04] did for ODSC. Wanted to make a version
[00:00:06] for the rest of you. Now, three big
[00:00:09] things I want you to take away. One is
[00:00:12] Genai is a little bit different in terms
[00:00:13] of the technical nuances that's
[00:00:16] important to know if you're building
[00:00:17] applications. Second, I wanted to give
[00:00:20] everybody a basic intro workflow for how
[00:00:23] you can build an evaluation data set,
[00:00:25] how you can do evaluation with
[00:00:27] generative AI. And finally, I'm hoping I
[00:00:30] can inspire all of you to go out and
[00:00:32] actually start doing evaluation because
[00:00:34] in the world that's how you actually
[00:00:36] learn this stuff is by doing, not
[00:00:38] reading papers or watching videos like
[00:00:41] that. Now, in the beginning, I always
[00:00:44] like to kind of motivate these talks
[00:00:45] with a funny anecdote. And the idea here
[00:00:48] is one of the biggest use cases for
[00:00:50] Genai is helping to write those customer
[00:00:53] support um tickets, emails out there.
[00:00:57] Anybody can go grab a little bit of a
[00:00:59] prompt, do a little vibe coding, have
[00:01:02] have a model write out a Genai uh output
[00:01:06] so you can get a customized response
[00:01:09] like this. This stuff works really good
[00:01:12] out of the box. And you're probably
[00:01:14] thinking, "Hey, I'm done." If you're
[00:01:16] building that quick demo app, you're
[00:01:18] good. But if you're in this game long
[00:01:22] enough, you'll see that wait a minute,
[00:01:24] those things don't always work, right?
[00:01:26] like I asked about a order delay, but my
[00:01:29] email tells me about a new product line,
[00:01:31] right? It doesn't match at all. Or
[00:01:34] I somebody asks about a specific order
[00:01:36] and we tell them that that they're
[00:01:39] they're sorry that their espresso
[00:01:40] machine arrived effective. Well, wait a
[00:01:42] minute. We don't actually sell espresso
[00:01:44] machines. There was an hallucination
[00:01:46] here that happened.
[00:01:48] And this is not isolated incidents.
[00:01:51] Cursor, the folks that build you that
[00:01:54] wonderful IDE, well, they were using a
[00:01:56] customer support bot. And guess what? It
[00:01:58] created a policy that said you're only
[00:02:00] allowed one device per subscription. And
[00:02:02] the reality is that wasn't right at all.
[00:02:04] You could use multiple ones. And this
[00:02:06] led people to like mistake cursors
[00:02:08] policy and actually cancel their
[00:02:10] subscriptions like that. Besides that,
[00:02:13] Genai has always been problematic around
[00:02:16] the inputs and outputs and whether
[00:02:18] they're copyright issues. We also know
[00:02:20] if you're using these Genai bots and
[00:02:22] having them out there that courts might
[00:02:26] be held that it's no different than an
[00:02:28] employee. If your Genai bot does
[00:02:30] something or creates a promise, well,
[00:02:32] you got to back that just like if your
[00:02:34] customer, your employee did that. So,
[00:02:36] there is a huge set of mistakes that you
[00:02:38] have to be worried about which is should
[00:02:40] cause you pause as somebody that's going
[00:02:43] in and building these systems. Now,
[00:02:46] what's happened and what's different in
[00:02:48] 2025 is it's not just you that's
[00:02:52] thinking about this. It's the executives
[00:02:54] because they've seen the numbers.
[00:02:56] They've seen how many Gen AI projects
[00:02:58] that they've invested into that haven't
[00:03:00] had huge returns. The 95% is way
[00:03:03] overstated, but still Gen AI can be
[00:03:08] useful, can have some good results, but
[00:03:10] it's not always a sure thing that it's
[00:03:12] going to be massive like that.
[00:03:14] So where evaluation comes in is it helps
[00:03:17] you build better Genai applications.
[00:03:20] I've been in the space for a long time.
[00:03:22] About two years ago I did this video
[00:03:24] been widely seen around doing
[00:03:26] evaluation. It's still a good video. A
[00:03:28] lot of good points here. But this is a
[00:03:31] little bit more condensed version of
[00:03:32] that video with a little fresher content
[00:03:35] for all of you all like that. And I
[00:03:37] always like to start with why evaluation
[00:03:39] is important. It's important if you're
[00:03:41] for yourself to build a better
[00:03:42] application to understand what's working
[00:03:44] and what isn't. But then you need to
[00:03:46] also convince your team, your managers.
[00:03:49] So you need to be able to show them
[00:03:50] like, hey, I ran this data set on things
[00:03:52] that hadn't seen before and look, it's
[00:03:54] holding up okay, as well as maybe you
[00:03:57] need to go convince regulators or a
[00:04:01] third-party um model evaluation team.
[00:04:04] Well, you're going to have to have eval
[00:04:06] data sets. You're going to have
[00:04:07] methodologies that you need to be able
[00:04:09] to show them that your app is going to
[00:04:11] do what you want it to and you have
[00:04:13] confidence and trust that it's going to
[00:04:15] perform in this and evaluation covers a
[00:04:18] lot of stuff. I know as data scientists
[00:04:21] we end up often talking about this
[00:04:22] technical piece. One thing I like to
[00:04:25] stress is you also have to think about
[00:04:27] the value of your applications. What's
[00:04:30] the ROI that's going to do it? At the
[00:04:33] same time, keep in mind the operational
[00:04:35] costs as well. What is the cost of
[00:04:37] ownership? This is one of the issues
[00:04:39] with, for example, GPUs and enterprises
[00:04:42] where it costs a lot to have a locally
[00:04:45] hosted open-source model that you're
[00:04:47] running 24/7 inside an enterprise, which
[00:04:49] is often why people go to APIs. But
[00:04:52] these types of considerations of what is
[00:04:54] it going to cost to actually run your
[00:04:55] solution on a day-to-day is an important
[00:04:58] thing that you need to think about at
[00:05:00] the outset of your of your application.
[00:05:05] Now, I think at this point we're all
[00:05:06] mature enough and we realize that
[00:05:08] companies out there promote their LMS
[00:05:10] with these public benchmarks. We know
[00:05:12] these public benchmarks, they're great
[00:05:13] for like giving you a rough performance
[00:05:15] of the LMS, but you're not going to use
[00:05:17] them to figure out is this a good fit
[00:05:19] for your application. Instead, you want
[00:05:22] to build benchmarks, data sets that
[00:05:24] reflect your case like that. So, let's
[00:05:28] jump in. Let me help you kind of tame
[00:05:31] generative AI. Now, we're going to start
[00:05:34] with the basics of how generative AI
[00:05:36] works. We're going to build an
[00:05:37] evaluation workflow and then add more
[00:05:39] complexity. And finally, we'll get to
[00:05:41] the more agentic stuff. When I gave this
[00:05:43] talk at ODSC, we ended up with so many
[00:05:45] questions. That's the advantage of you
[00:05:47] catch me in person that we didn't get to
[00:05:49] the later parts. I get to cover the
[00:05:50] later parts here in the video since none
[00:05:52] of you can. I'm not going to stop for
[00:05:54] any of your questions right now. So, the
[00:05:58] reason why Genai is so hard is is if you
[00:06:00] write a prompt like this,
[00:06:03] you're going to go through and you're
[00:06:04] going to get an output. But here's the
[00:06:06] same thing. If you just wait a minute or
[00:06:08] two, your system's going to give you a
[00:06:11] different output. Now, in this case, the
[00:06:14] outputs are substantively the same,
[00:06:16] right? They're covering the same types
[00:06:18] of materials around investigations and
[00:06:20] help. But you see there are slight
[00:06:22] differences. And when we get to
[00:06:24] evaluation, this becomes an important
[00:06:27] consideration for us. And I want to
[00:06:30] break this up because there's a lot of
[00:06:32] places where this happens. And we're
[00:06:34] going to just systematically go through
[00:06:36] it. We'll talk about inputs, the model,
[00:06:38] and then output.
[00:06:41] So the first thing is
[00:06:44] is we see some inconsistent scores
[00:06:46] around benchmarks. And this is a story I
[00:06:48] told many times. I still like to tell
[00:06:50] it. I worked at Hugging Face. Tom Wolf,
[00:06:52] one of the co-founders, loves to talk
[00:06:54] about new great open source models.
[00:06:57] Tweet it out. Let everybody know, look
[00:06:59] how good it is. Look at the performance
[00:07:01] that we see across all of these
[00:07:03] benchmarks. Well, in this case, when he
[00:07:06] tweeted it out, somebody else tweeted
[00:07:07] back, "Wait a minute. How come the
[00:07:10] numbers you're tweeting out differ from
[00:07:12] the ones that are actually in the llama
[00:07:14] paper?" Now, in both of these cases,
[00:07:16] you're using the same data set, as we'll
[00:07:20] talk about MMLU, but why are we showing
[00:07:22] two different numbers? Now, MMLU is a
[00:07:25] multiplechoice benchmark. It covers like
[00:07:28] history, economics, biology, like
[00:07:30] questions that like high schoolish kind
[00:07:32] of colleges questions around knowledge
[00:07:34] in these areas. So, it's often used to
[00:07:36] measure like how kind of smart a model
[00:07:38] is um like that.
[00:07:42] But why would two different models get
[00:07:44] different scores on a multiple choice
[00:07:46] test? Like it doesn't make any sense if
[00:07:48] you think about it. [gasps]
[00:07:50] Well, it turns out if we dig into it
[00:07:52] that if we look at the three evaluation
[00:07:55] harnesses that were used, three
[00:07:56] different approaches, they each wrote
[00:07:59] their prompts slightly different. If you
[00:08:01] take a look here, you'll see that
[00:08:02] there's slight differences in the prompt
[00:08:05] where one of the the how they phrased
[00:08:06] it. Some use the word question. Wait,
[00:08:09] you see the word choices here. All of
[00:08:12] this ends up affecting the overall
[00:08:14] accuracy of the how we evaluate the
[00:08:18] model because it leads to changes in the
[00:08:20] output. And look, this isn't crazy stuff
[00:08:22] like Anthropic wrote about this too, how
[00:08:25] just changing the options from
[00:08:27] parenthesis a to parenthesis one or
[00:08:29] going from round parenthesis to square
[00:08:31] parenthesis had an effect on the output.
[00:08:34] Now, this hasn't changed recently. If we
[00:08:38] look at, for example, GPT40,
[00:08:40] [snorts] still very sensitive to the
[00:08:42] prompts that are there. We've seen this
[00:08:44] in a number of recent studies where how
[00:08:46] you change the just the tone of what
[00:08:48] you're doing can affect the overall
[00:08:50] accuracy. So, how you ask something can
[00:08:53] do that, right? Like, I guess this is
[00:08:55] why mom always had to be polite, right?
[00:08:57] Um, and it's still a huge piece. you
[00:09:00] still see um posts like this that tell
[00:09:03] you, hey, use specific phrases, do this
[00:09:05] because these models still even years
[00:09:08] later are still kind of sensitive like
[00:09:10] that. Hopefully, you know, two years
[00:09:12] from now this won't be an issue, but for
[00:09:13] now it is.
[00:09:16] So a second story I like to tell and
[00:09:18] this is about the Falconella model is
[00:09:21] when it was first released people were
[00:09:23] trying out different things and part of
[00:09:24] it is right it came out from the Middle
[00:09:26] East and some people were concerned like
[00:09:28] hey wait a minute like could this be
[00:09:30] biased in some ways and they tried
[00:09:32] different um different queries and one
[00:09:36] of them said you know recommend me a
[00:09:37] technological city and why did it
[00:09:39] recommend Abu Dhabi like did they train
[00:09:41] it themselves they can change the input
[00:09:44] data into this Did they alter the
[00:09:46] weights or something? People were a
[00:09:49] little confused and there was even
[00:09:50] concerns like that maybe this is
[00:09:52] covering up human rights issues because
[00:09:53] it gives different answers for different
[00:09:56] cities like that. Well, someone on the
[00:09:58] team kind of dug into this and like
[00:10:01] anything when you start digging into
[00:10:02] this do you jump right into like looking
[00:10:04] at model weights or no anything? No. You
[00:10:06] look at what's the inputs into the
[00:10:08] model? Well, besides your query, a
[00:10:11] system prompt. And it turns out that the
[00:10:14] system prompt here gave the location. It
[00:10:17] said, "Hey, you're a model that was
[00:10:18] built in Abu Dhabi." And so that was
[00:10:21] leading these queries to kind of use Abu
[00:10:24] Dhabi in a different way than other
[00:10:26] cities. And it's just a reminder that
[00:10:28] you need to think about the system
[00:10:30] prompt whenever you're working with
[00:10:31] these models. [snorts] And I can
[00:10:33] guarantee you most of you have not.
[00:10:35] Right? If you go look at the cloud
[00:10:37] system prompt, it'll take you like eight
[00:10:39] or nine minutes if you just want to read
[00:10:41] that straight through. And I'm sure very
[00:10:43] few people have done that. But those
[00:10:46] 1700 words in the prompt can have an
[00:10:49] effect and it could be a different
[00:10:50] effect depending on the query that
[00:10:52] you're asking that could affect that
[00:10:54] response of the model. So as you start
[00:10:56] using these models more, you need to
[00:10:58] spend time looking at those system
[00:11:00] prompts as well. So these are just some
[00:11:03] of the things just on the input side
[00:11:06] that can affect it. Now, when we get
[00:11:08] over to the model side, you got to
[00:11:10] remember these models are all very
[00:11:13] different. Even if you take two models
[00:11:15] that are very similar. So, if we take a
[00:11:17] look here, and I don't think I can show
[00:11:19] it with my mouse properly. I'll hover
[00:11:21] it, but we'll see if it shows up. But
[00:11:24] take a look at two models that are very
[00:11:25] similar to each other. The Llama 70B and
[00:11:28] the Llama 3B or 8B model.
[00:11:33] Very high tech similarity, but they're
[00:11:35] not perfect. even though they've
[00:11:36] probably been trained with very similar
[00:11:39] if not exactly the same data right the
[00:11:41] AB just had a little bit less of it
[00:11:44] and so I hope this gets the red light
[00:11:46] that these models when they give their
[00:11:48] outputs just the nature of the design of
[00:11:51] them even if they're the same family is
[00:11:53] going to lead to different types of
[00:11:55] outputs and this is always a
[00:11:56] consideration when you're moving from
[00:11:57] one model to another you always have to
[00:12:00] run your eval to make sure that
[00:12:02] everything holds up I can tell you on
[00:12:04] our team we've seen sometimes times
[00:12:05] going from two to 2.5 actually we can
[00:12:08] see a regression in lots of behaviors
[00:12:10] where we have to go back and change the
[00:12:12] prompts that we're using.
[00:12:14] These models have a lot of their own
[00:12:16] nuances. Some of these models for
[00:12:18] example can be very syo sickopantic. I
[00:12:20] love using that word. Right? They can be
[00:12:22] overly nice. They can love you too much.
[00:12:24] We saw this when when OpenAI released
[00:12:26] GPT41 and then they pulled that back
[00:12:28] because it was just way over the top
[00:12:30] like that. This is a consideration is
[00:12:33] when with your prompts besides that when
[00:12:37] you're using commercial APIs and you
[00:12:38] don't have control of that model and the
[00:12:40] weights well things can change over time
[00:12:44] they can update those models without
[00:12:46] telling you and people have been
[00:12:47] measuring this and seen this type of
[00:12:49] drift like that and there's lots of
[00:12:52] reasons for it but um Anthropic recently
[00:12:55] released a post where they showed that
[00:12:57] you know some of the things they had
[00:12:58] some very technical reasons for why
[00:13:00] things were going wrong like context
[00:13:02] windows output corruption, but these
[00:13:04] lasted for days and degraded and changed
[00:13:07] the outputs of your models. So things
[00:13:09] you need to be aware of when you do
[00:13:11] this. And I'll tell you if you think
[00:13:13] you're just using open source and you
[00:13:15] can control it kind of, but the VLM the
[00:13:18] the inference endpoint that you use,
[00:13:21] somebody could have updated that and
[00:13:23] changed that. So even with the weights
[00:13:24] the same, you have to think about the
[00:13:26] entire infrastructure stack when you
[00:13:29] want to make sure things are the same.
[00:13:31] And of course there's always things like
[00:13:33] hyperparameters that you can go and
[00:13:35] modify. You can modify right your top P,
[00:13:38] your temperature. You can set
[00:13:40] temperature one for the models to be
[00:13:42] very creative. You can set it all the
[00:13:44] way down zero because you don't want the
[00:13:46] model creative. You want it to take the
[00:13:48] biggest thing out of its probability
[00:13:50] like that. These are all things that you
[00:13:52] can control, but they can affect the
[00:13:54] output. Now, one recent thing, and if
[00:13:58] you're not used to GPUs, this is a
[00:14:00] little bit new, is
[00:14:03] often the way we implement these these
[00:14:08] LM models.
[00:14:10] They're non-deterministic inference in
[00:14:12] practice. So with a traditional XG boost
[00:14:15] model, I can set my seed and then my
[00:14:19] inputs, I should always get the exact
[00:14:21] same outputs. It's deterministic
[00:14:24] here. Even if you set the seed, even if
[00:14:26] you give the same inputs, you can have
[00:14:28] differences. Then there's a number of
[00:14:30] different reasons for this and this is
[00:14:32] why I give lots of people. It it can be
[00:14:35] the floating point error can accumulate
[00:14:38] because often it's not just your your
[00:14:40] floating point. It's combining with
[00:14:42] others that can cause slight rounding
[00:14:44] differences can can do that. There's
[00:14:47] also things with mixture of experts
[00:14:49] models where you'll have different
[00:14:50] batches, different activations for that.
[00:14:53] And so the reality is is it's very
[00:14:56] prevalent to have non-deterministic
[00:14:58] inference around this. And you'll look
[00:15:01] if you look carefully at many of the
[00:15:02] providers, they'll have a page in there.
[00:15:04] They're saying, "Hey, you we're not
[00:15:06] being able to guarantee determinism."
[00:15:08] Now, and this is why we always watch
[00:15:11] these videos. We've had a recent update.
[00:15:13] The folks over at Thinking Machines
[00:15:15] spent a little bit of time and figured
[00:15:16] out, hey, if you correctly batched
[00:15:19] things, you can try to defeat this
[00:15:22] non-determinism and have have it
[00:15:24] deterministic. There was, I believe, a
[00:15:27] slight latency hit for this approach,
[00:15:30] but I just saw in the last week or two
[00:15:32] that it's been introduced in VLM. So,
[00:15:35] there may be a chance, but you'll have
[00:15:37] to check with your folks to see if
[00:15:38] you're able to get deterministic
[00:15:40] inference um with that. But for the most
[00:15:43] part, I think you all of you should be
[00:15:44] aware of the non-determinism problem
[00:15:46] with GPUs.
[00:15:49] So, that's that's it. We've covered the
[00:15:51] model. There's still one more
[00:15:53] interesting thing. Even if all those are
[00:15:56] the same, changes in the outputs can
[00:15:58] change that. And here what I like to do
[00:16:01] is go back to MMOU because there we were
[00:16:04] trying to get a multiplechoice output.
[00:16:07] And you're probably like, Raj, multiple
[00:16:08] choice output, four choices. How tricky
[00:16:11] could this be? Well, let me tell you,
[00:16:15] how are you going to tell your model to
[00:16:17] select a choice? Are you going to tell
[00:16:20] it select the first letter? when you
[00:16:23] think about it or are you gonna tell it
[00:16:26] to look at the entire answer and be able
[00:16:29] to come up with a determinant
[00:16:32] there's more than one way to kind of do
[00:16:34] this or you can limit it to one of the
[00:16:35] existing choices and in fact we see this
[00:16:38] with different implementations for the
[00:16:40] MMLU where we saw that these models were
[00:16:44] doing different ways right gu just fix
[00:16:47] it to those four letters and that's it
[00:16:50] have it generate the text for the and
[00:16:52] use the first better compare the full
[00:16:54] answers like that. And this has a
[00:16:56] meaningful effect on the overall
[00:16:58] performance about which approach you
[00:17:00] use. And so this again shows you the
[00:17:03] value of understanding this and
[00:17:04] recognizing this. And I shared a
[00:17:07] spreadsheet which this is a little bit
[00:17:09] dated but had a bunch of open source LMS
[00:17:12] where they use the exact same prompt and
[00:17:14] you can see just a humongous variety in
[00:17:16] the output of these models like that.
[00:17:20] And recently now we don't just look at
[00:17:23] outputs with reasoning models are making
[00:17:25] decisions. For example, deciding what
[00:17:28] tool to use under what conditions. And
[00:17:31] this is another area where we're going
[00:17:32] to see a lot of non-determinism where
[00:17:34] different models at different times
[00:17:35] you're going to pick different tools.
[00:17:37] And I've seen it internally in our own
[00:17:38] things is sometimes it uses the tools,
[00:17:40] sometimes it doesn't. That can be a
[00:17:43] little bit tricky when you're building
[00:17:44] these workflows as we'll talk about um
[00:17:46] like that. So I tried to shove all this
[00:17:49] stuff into one slide. I hopefully by
[00:17:51] walking through it you this is a little
[00:17:53] bit more palatable and understandable
[00:17:56] but this is one big takeaway I want all
[00:17:58] of you to understand if you're not
[00:18:00] familiar with everything that I talked
[00:18:01] about. These are all pieces that you
[00:18:04] should understand how these stacks work
[00:18:07] because that way you can make sure
[00:18:09] you're going to get consistent outputs
[00:18:10] and none of these little things are
[00:18:12] going to trick up when you're working on
[00:18:14] your applications like that.
[00:18:17] So, I know it feels chaotic. I've showed
[00:18:20] you a lot of stuff. Haven't made you
[00:18:21] feel better. And don't worry, we got a
[00:18:23] lot of tools up our sleeve that we're
[00:18:25] going to be able to do this. And let's
[00:18:27] walk through. Now, we're going to spend
[00:18:29] a big chunk of time. How do we do a simp
[00:18:32] how do we do how do we evaluate a Genai
[00:18:34] app? Now, as a starting point, you're
[00:18:37] going to have some type of prompt.
[00:18:39] You're asking the model to do something,
[00:18:41] right? A summarization, extract a city.
[00:18:44] Then what you're going to need to do is
[00:18:46] get some labeled output. We call it
[00:18:49] labels. We call it output. Called a
[00:18:51] reference. Called it gold. A lot of
[00:18:53] different words for this. But you need
[00:18:55] this ground truth, another word for it
[00:18:58] to to help doing that because you use
[00:19:00] that ground truth to compare to your
[00:19:02] model output. And that tells us are they
[00:19:05] consistent? Are they lined up? Now,
[00:19:08] ideally, if you have enough humans, you
[00:19:10] would just have humans check all of
[00:19:12] these all the time. But usually humans
[00:19:15] are too expensive and too tired to work
[00:19:16] all the time. So a common technique is
[00:19:20] we use an LM that reads your prompt that
[00:19:23] reads the outputs and says are these
[00:19:26] things the same or not. As we'll talk
[00:19:28] about this works pretty well. Now I'm
[00:19:31] not asking for an exact lexical string
[00:19:33] match. I'm just saying hey do these
[00:19:36] things say the same? And you can control
[00:19:37] the prompt for the LM judge with a
[00:19:39] little bit. So for example, these two in
[00:19:41] this case we're going to treat as equal.
[00:19:45] This makes it a lot easier to run your
[00:19:47] evaluation by leveraging that the
[00:19:51] strength and the [clears throat] power
[00:19:52] of kind of an automated judge to do
[00:19:55] that. Now, if you use that equivalence
[00:19:57] that I was talking about as your metric,
[00:19:59] you can almost just treat it like
[00:20:02] traditional machine learning where you
[00:20:03] have a hyperparameter where now all
[00:20:05] you're trying to do is change the knobs
[00:20:08] in your in um in your application to
[00:20:12] maximize that equivalence. And you'll go
[00:20:14] through and you'll be like, okay, I'll
[00:20:16] change this prompt. Did my equivalence
[00:20:17] go up? Oh, I changed this model. Did my
[00:20:19] equivalence go up? That's how you can
[00:20:21] then try to help improve your model.
[00:20:24] That's [laughter] it. That's a very
[00:20:26] simplified way of how to do all of this.
[00:20:28] And this is the good part. It's a lot
[00:20:30] like classic ML evaluation. Now, the bad
[00:20:33] part about this is like you can see like
[00:20:35] you can't stuff the entire dragon in the
[00:20:37] box because there's things we're missing
[00:20:39] here. Okay? There's other aspects of the
[00:20:41] answer that we're not catching when we
[00:20:43] just focus on this equivalence thing.
[00:20:45] The other part is sometimes it's really
[00:20:48] hard to generate that gold answer. And
[00:20:51] so I want to kind of touch upon that as
[00:20:53] well.
[00:20:55] And so here what we want to do is go
[00:20:58] into a more targeted evaluation where
[00:21:02] I've shown you the stuff you can get
[00:21:03] quickly out of the box. But if we spend
[00:21:05] more time with the data, spend more time
[00:21:07] understanding, we can up the accuracy of
[00:21:11] our models. And what this involves is
[00:21:14] taking time to build tests.
[00:21:17] Now to building tests, what we need to
[00:21:20] do is dig deeper into the problem set.
[00:21:23] So here I have an example. Look, it
[00:21:26] looks good right away, right? It's got a
[00:21:29] nice answer for that. While this
[00:21:32] response here, we're going to call the
[00:21:34] bad example.
[00:21:37] Now, I'm going to ask you, why are we
[00:21:38] calling this the bad example? What is it
[00:21:40] about it that's bad? H, and maybe some
[00:21:44] of you can think about it, but this is
[00:21:46] the one of the biggest things I want you
[00:21:48] to learn and take away is to understand
[00:21:51] what's bad about this. to understand
[00:21:53] what's wrong with one of your evaluation
[00:21:55] sets, you need to go talk to those
[00:21:58] experts. One of the biggest problems I
[00:22:01] see and especially I know as data
[00:22:04] scientists, AI developers, we want to go
[00:22:06] find an optimal solution. We want to
[00:22:10] just look for an archive paper. We want
[00:22:13] to look for an algorithm. We don't want
[00:22:15] to leave our chair. We just want to find
[00:22:17] an answer by ourselves. But evaluation
[00:22:20] is all about solving problems. And we
[00:22:22] need to go out and talk to others. And
[00:22:26] having this collaboration with the
[00:22:28] users, with the domain experts is a
[00:22:30] must. And this is where if you want to
[00:22:32] succeed in this field, if you want to
[00:22:33] build successful applications that
[00:22:35] people are happy with, you got to go out
[00:22:37] there, talk to the domain experts, talk
[00:22:39] to the users, pretend you're a naive
[00:22:41] user, and kind of bootstrap your up. But
[00:22:44] this is a big part of doing this. You're
[00:22:47] not going to just solve this with
[00:22:48] formulas and listening to videos like
[00:22:50] me.
[00:22:52] Because once you have this knowledge
[00:22:53] then you can go through all those
[00:22:55] examples that we showed earlier and you
[00:22:57] can be like oh yeah these things are
[00:22:59] related to each other. These this is
[00:23:00] this is a group a a a cluster a type of
[00:23:03] behavior we see and the light bulbs
[00:23:05] start going off and you start to see
[00:23:07] patterns in your data. And then when I
[00:23:11] say what's that bad example is you can
[00:23:12] be like oh yeah well you know what it's
[00:23:14] too short right the tone it lacks
[00:23:16] professional you can explicitly kind of
[00:23:19] start to tell me what's wrong and you
[00:23:21] need this because once you have that
[00:23:24] explicit knowledge once you know exactly
[00:23:26] why then what we can do is start to
[00:23:29] build tests so what I want you to do is
[00:23:32] when you have those prompts the models
[00:23:34] has their responses have your humans
[00:23:37] evaluate those model responses. Have
[00:23:40] them go through. Now, you're not going
[00:23:41] to do everything, but even a small
[00:23:43] subset, have them write down what they
[00:23:45] think is good about that response and
[00:23:47] what is bad because you can then use
[00:23:49] that to start to build tests. Now,
[00:23:53] sometimes you can do this in Excel. You
[00:23:55] can build custom evaluation tool. Don't
[00:23:57] go overboard with building custom
[00:23:58] evaluation tool, but you can make it
[00:24:00] easier to work with your experts like
[00:24:02] that. But here, for example, we talked
[00:24:05] about, oh, it's too long. Well, you know
[00:24:07] what? We can build a simple test in
[00:24:10] Python, right? We don't need a fancy
[00:24:12] model. This is just basic Python that
[00:24:14] says, hey, you know, is this something
[00:24:17] over under eight words or under over
[00:24:19] 200? Let's flag that.
[00:24:22] I could build another test which looks
[00:24:24] at the tone and style. And here I'm
[00:24:27] going to use an open AI model. I'm going
[00:24:29] to use that LM as a judge to see what
[00:24:32] was the style and tone because these
[00:24:35] models can effectively do that. And when
[00:24:38] I do that now, when I run my when I run
[00:24:41] my prompts through,
[00:24:44] I can look at the response that was
[00:24:45] given
[00:24:48] and then I could have this automated
[00:24:49] test which tells me which answers passed
[00:24:52] the length test, which ones didn't,
[00:24:55] which ones passed the tone test like
[00:24:57] that. So now these give are my new
[00:25:00] automated tests that help me I figure
[00:25:03] out where my failures are. [snorts]
[00:25:07] Now, as I'm working on this, one step
[00:25:09] further I can always do is as I'm
[00:25:12] building tests, we still want to use
[00:25:14] equivalence, but let's make sure that
[00:25:17] equivalence
[00:25:18] is lined up with our human evaluation
[00:25:22] because the whole idea of that
[00:25:23] equivalence is we just want to take the
[00:25:25] burden off our humans to check every one
[00:25:27] of these. So, it might involve slightly
[00:25:30] changing the prompt, changing changing
[00:25:33] that equivalence judge so it aligns,
[00:25:35] right? aligns with that human. And if
[00:25:38] they're looking at the world
[00:25:38] differently, it's not very helpful. You
[00:25:40] need them to be aligned to do that.
[00:25:44] Now,
[00:25:46] [clears throat]
[00:25:47] now, and I know that this is where we
[00:25:49] got a lot of questions at,
[00:25:52] now when you're doing this, one of the
[00:25:54] things to be careful about is you're
[00:25:56] going to use you're using different
[00:25:58] judges for different pieces here. As we
[00:26:00] talked about, you're have that
[00:26:01] equivalence judge that we talked about.
[00:26:03] You might use other judges.
[00:26:06] These models, they love themselves.
[00:26:10] If you give a GPT4 model text, it likes
[00:26:13] its GPT4 text better than text from
[00:26:16] other models. So, just be aware if
[00:26:19] you're using models together that are
[00:26:22] the same, they're going to like
[00:26:24] themselves better better. It if to be to
[00:26:28] get them a little more critical, you
[00:26:30] want to mix it up. You want to use Cloud
[00:26:31] for one, GPT4 for another, Gemini for
[00:26:33] another. So, this is just a little
[00:26:35] trick. It's not going to make a huge
[00:26:37] different, but it can make enough of a
[00:26:38] subtle difference that you want to think
[00:26:40] about kind of making sure you don't
[00:26:42] limit yourself to this self-evaluation
[00:26:44] bias.
[00:26:46] We talked about the alignment piece. You
[00:26:48] always want to make sure that the humans
[00:26:50] that are checking your things, and you
[00:26:52] should be having humans spot check at
[00:26:54] the beginning and all the way through
[00:26:57] that they're lined up with what your LLM
[00:26:59] judges are doing. Now, these LM judges,
[00:27:02] we talked about the self-evaluation
[00:27:03] bias. They have lots of other biases.
[00:27:06] So, as if you're somebody getting into
[00:27:08] this field, you should take a look at
[00:27:09] these biases. You should slowly learn
[00:27:11] this because this is going to affect
[00:27:13] your evaluation results over time. If
[00:27:15] you don't, for example, know that LM for
[00:27:17] example favor the early answers over the
[00:27:20] later answers. So really, you need to
[00:27:21] scramble up your answers every once in a
[00:27:23] while or otherwise that bias is going to
[00:27:25] carry through towards your evaluation
[00:27:28] like that.
[00:27:31] All right, we're making good progress
[00:27:32] here. So now that you've done this,
[00:27:37] what we can do is start to collect the
[00:27:39] results that you have and figure out
[00:27:42] what are the patterns that we're seeing.
[00:27:44] And this is error analysis. I can go
[00:27:46] through and see, oh look, when I did
[00:27:49] GPT4 or when I did GPT3, how many
[00:27:52] failures did I get in tone? How many did
[00:27:53] I get with GPT4? Right? This helps me
[00:27:56] figure out like, oh, I can compare these
[00:27:58] two to see what's going on. Right? Or I
[00:28:00] could compare two different prompts.
[00:28:02] This is the value of having all these
[00:28:04] test cases. Now I have a better
[00:28:06] understanding of where my failures are
[00:28:08] and how to fix them like that. One of
[00:28:11] the other tips you can do is when I use
[00:28:14] equivalence, I always like to ask for an
[00:28:16] explanation for equivalence. I just ask
[00:28:18] it to give me one sentence to explain
[00:28:21] explain its decision. That helps me a
[00:28:24] ton. Like these models are really useful
[00:28:26] if you have them explained. It just kind
[00:28:28] of helps you focus your thinking about
[00:28:30] what are the relevant pieces of
[00:28:32] information for why it made its
[00:28:33] decision. Now, the explanations aren't
[00:28:35] exactly what the model's doing. Like
[00:28:37] don't don't go too far with that. It's
[00:28:38] just a huristic to help you understand
[00:28:41] what's going on and make quicker sense
[00:28:43] of this. So all of this is really this
[00:28:46] kind of evaluation flywheel. My buddy
[00:28:48] Hamill, he's done a course. If you find
[00:28:50] him on X or Twitter, he's he talks about
[00:28:51] this stuff all day long. But this is the
[00:28:54] idea for what you're going to do. You're
[00:28:57] going to go build that evaluation.
[00:28:58] You're going to analyze things. But then
[00:29:01] you're gonna build these tests, see how
[00:29:04] they are, [clears throat] figure out
[00:29:06] where your weaknesses are, improve it,
[00:29:08] build that evaluation, spend some time
[00:29:10] with the data. Meet this is a flywheel
[00:29:12] cycle that you're going to continually
[00:29:14] do to do that. [snorts] So, let me show
[00:29:17] you one more example of how we even go
[00:29:19] crazier with this. Um, suppose you're
[00:29:22] building a financial analyst agent and
[00:29:24] you care about the style. And this is a
[00:29:26] real world use case. This is happens all
[00:29:28] the time. people are very particular how
[00:29:31] their their responses should be like
[00:29:34] that. Now, when you assess the response
[00:29:37] here, and you can see the response here,
[00:29:38] it's it's kind of long. It's not like a
[00:29:40] one-s sentence kind of thing. You could
[00:29:43] write a global LM test that says, "Hey,
[00:29:46] was this explained as I would expect for
[00:29:48] like a financial analyst at a regulated
[00:29:50] firm?" It's one approach you could do.
[00:29:52] What I want to introduce you to, we've
[00:29:54] talked about tests, we can build unit
[00:29:57] tests. So suppose I think of style as
[00:30:01] composed of six elements. The context,
[00:30:03] the clarity, precision, compliance,
[00:30:05] actionability, and risks. I can build a
[00:30:08] unit test for each one of those. And so
[00:30:11] what this allows me to do is now when I
[00:30:14] want to check a query, I can go and just
[00:30:16] look at the results of the unit tests.
[00:30:19] And that gives me an idea of where where
[00:30:22] the answer is working well and where it
[00:30:23] isn't. versus if I just use that global
[00:30:26] test, like there's a lot of stuff for me
[00:30:28] to read through to figure out exactly
[00:30:30] like where the model is. And the great
[00:30:33] thing with the unit test is you can even
[00:30:35] combine them, cluster them together, and
[00:30:37] find patterns in them. And I have a
[00:30:39] whole notebook and a whole worksheet
[00:30:40] kind of showing you how to do this. But
[00:30:42] hopefully the light bulbs are going off
[00:30:44] here. As you look for errors, you can
[00:30:46] group them, look for larger clusters of
[00:30:49] errors along the ways like that. All
[00:30:52] right, a little bit on unit tests, but
[00:30:55] what I want to do is keep moving on.
[00:30:57] Lots of good things you can do with unit
[00:30:59] tests.
[00:31:02] All right.
[00:31:04] So, all of this so far kind of comes
[00:31:06] under the bundle of air analysis of of
[00:31:09] what we want to do. Now,
[00:31:12] you guys don't get all my fun jokes.
[00:31:14] What I want you to do is make sure that
[00:31:17] when you're thinking about error
[00:31:18] analysis, you change one thing at a
[00:31:21] time. If you try to change too many
[00:31:23] things and it's very very tempting,
[00:31:25] right? There's like four or five, right?
[00:31:26] There's so many settings. Change one
[00:31:28] thing at a time. See how what the effect
[00:31:30] is
[00:31:32] as you're going through, as I've shown
[00:31:34] you before already. We categorize the
[00:31:36] failures. We keep track of it. As you
[00:31:39] find examples that are outliers, edge
[00:31:41] cases, or really good examples, like
[00:31:44] heart them, save them, favorite them,
[00:31:46] find a way to keep track of them because
[00:31:47] you're going to want to come back in
[00:31:48] them in a few days. And if you haven't
[00:31:50] done that, it's a pain in the butt to
[00:31:51] try to go find it. Finally, use some
[00:31:54] type of tool that gives you logs,
[00:31:56] traces, so that way you can go back and
[00:31:58] do that investigation as well. There's a
[00:32:00] ton of them, as we'll talk about, on the
[00:32:02] market.
[00:32:04] Now, as you go through this, if you
[00:32:06] haven't done this stuff before, you
[00:32:08] often see people tell a story like this,
[00:32:10] like, "Hey, we started off, the
[00:32:12] performance wasn't that great, but then
[00:32:13] we started tweaking some things. We're
[00:32:15] able to get to a really good performance
[00:32:17] where we did it." And this is this nice
[00:32:20] like linear history of the natural
[00:32:22] progression of what happens. Let me tell
[00:32:24] you, they that's not actually how it
[00:32:27] happens. The reality is is it's often
[00:32:30] you do a couple steps forward, you fall
[00:32:32] back, you do a couple steps forward, you
[00:32:33] fall back. Like it's not in like that.
[00:32:37] So as you're working through this, if
[00:32:38] you get frustrated at times, no, just
[00:32:41] keep sticking with it. It will get
[00:32:43] better. And I think that's the
[00:32:44] difference between people who've done
[00:32:45] this for a while versus new people is
[00:32:47] people who've done this for a while know
[00:32:49] if you keep sticking with the approach
[00:32:51] that I told you will slowly improve the
[00:32:54] application and it will get better even
[00:32:56] though you might not see it at your
[00:32:58] current state like that
[00:33:01] when you're working with use cases in
[00:33:03] Gen AI an important consideration is to
[00:33:06] do user acceptance testing and this is
[00:33:08] because unlike traditional machine
[00:33:10] learning where you had a hold out data
[00:33:12] set that you could use to see if it
[00:33:14] generalizes.
[00:33:16] We don't really have that in Gen AI.
[00:33:17] Your hold out data set is your users.
[00:33:19] You have to involve them early on
[00:33:22] because otherwise your application won't
[00:33:23] work very well. So what I tell people is
[00:33:27] build that initial evaluation data set,
[00:33:29] spend some time trying to optimize on
[00:33:31] it, but then go test it with your users.
[00:33:34] Has to be that production settings
[00:33:35] because they're going to give you
[00:33:37] invaluable feedback that you didn't
[00:33:38] catch the first time. And then you're
[00:33:40] going to use that to update your eval
[00:33:42] data set, do some more hill climbing,
[00:33:45] and figure out what you improve. And
[00:33:48] then the cycle keeps going until you're
[00:33:50] happy enough where we want to go ahead
[00:33:52] and move it into production like that.
[00:33:56] Relatedly, as we get to these Genai
[00:33:58] applications, they're big. There's a lot
[00:34:00] to it. And so, one of my favorite quotes
[00:34:03] is when we're dealing with like these
[00:34:04] huge overwhelming tasks is, how do you
[00:34:06] eat an elephant? Right? You eat it one
[00:34:08] bite at a time.
[00:34:10] Same thing for Gen AI. Like the first
[00:34:12] time through, you're just basically
[00:34:14] seeing does my app work or not? Like not
[00:34:17] trying to catch every edge case. Over
[00:34:20] time, you'll continue to add tests as
[00:34:22] you notice, oh really, we need to focus
[00:34:23] on this point or oh, this point isn't.
[00:34:25] And so your initial app might just have
[00:34:27] a handful of tests, but six months
[00:34:30] later, if this app is useful, you've
[00:34:31] kept building on it, you might have 80,
[00:34:34] 90, 100 tests for that same application
[00:34:37] as well. So that's the normal flow.
[00:34:39] Don't try to do too much all at once
[00:34:42] like that.
[00:34:45] All right, we're making good progress
[00:34:47] here. Hopefully I've been going not too
[00:34:50] fast for you. Last thing I want to touch
[00:34:52] a gentic, right? It's the way the world
[00:34:55] is going. We know um we see so many of
[00:34:58] this
[00:35:01] and the the trouble with this is the the
[00:35:04] idea of the agentic is now the model's
[00:35:06] making decisions. it's making it's using
[00:35:08] its reasoning its tool calls to decide
[00:35:11] you know should I fly should I swim you
[00:35:13] know what other approach should I take
[00:35:15] in doing this and this giving the model
[00:35:17] this agency makes it much harder for us
[00:35:19] to track what's going on so here was a
[00:35:22] simple example I saw for kind of imagine
[00:35:25] a model that's going to look at the the
[00:35:28] input and then based on that input it's
[00:35:31] going to make a decision for what the
[00:35:32] customer wants do they need to search
[00:35:34] for a product do they need customer
[00:35:36] port? Do they want to track a package?
[00:35:39] Now once it decides which of these
[00:35:41] things the customer wants to do, this is
[00:35:43] that router function.
[00:35:45] The second step is to actually execute
[00:35:48] and do that complete workflow.
[00:35:51] So I want you to break down these tasks
[00:35:53] when you have it like this. Like what is
[00:35:54] that initial decision that's going on
[00:35:58] around which of these workflows we're
[00:35:59] going to follow and then we can look at
[00:36:02] the efficacy of each of those workflows
[00:36:04] and how well the model does upon that.
[00:36:06] This is the kind of breaking down that
[00:36:08] you have to do when you work with this
[00:36:10] stuff. You have to find ways to bite
[00:36:13] that elephant and make it easier.
[00:36:15] Similarly like Snowflake has this text
[00:36:17] to SQL agent seems really fancy but
[00:36:19] again you can kind of break this down
[00:36:21] into pieces where for example they have
[00:36:23] an initial piece that says hey should
[00:36:27] this is this a good question for us to
[00:36:28] take as a SQL agent and so they have
[00:36:31] this quick classification at the front
[00:36:33] end. We can build a test we can build
[00:36:35] and evaluate just this portion of it
[00:36:38] before we go in and look at the rest of
[00:36:40] the text to SQL like this.
[00:36:44] And one of the things you'll see is as
[00:36:46] you go into more of these agentic things
[00:36:48] is all of these LLM agents are doing
[00:36:50] different kinds of tasks here. They're
[00:36:52] finding files, finding actions, failing
[00:36:54] find failing to use tools. We can build
[00:36:57] and test and evaluate each one of these.
[00:37:00] And it's the same process we talked
[00:37:03] about where you're going to assess how
[00:37:04] well it's doing the routing, assess the
[00:37:07] individual age steps, see where it's
[00:37:09] working, where it isn't, categorize all
[00:37:11] of that. It's just a lot more but it's
[00:37:14] the same kind of basic action error
[00:37:17] analysis action and let me tell you I
[00:37:19] know it gets complicated right like this
[00:37:20] is a huge workflow which has lots of
[00:37:23] steps to do this and I think we're still
[00:37:25] figuring out our way for h what's the
[00:37:28] best approach as you build these more
[00:37:29] complicated workflows where you're
[00:37:31] you're not going to go in from the start
[00:37:33] and be able to evaluate every step of
[00:37:35] the process where maybe you look end to
[00:37:37] end for some maybe for some of them you
[00:37:39] actually break it down into micros and I
[00:37:41] think we're all just learning and trying
[00:37:43] to figure out how we do all of that. As
[00:37:46] you're doing this, one caution I have
[00:37:48] for people is there's a lot of agentic
[00:37:49] frameworks out there. They're great for
[00:37:51] demos, great to kind of get you started,
[00:37:53] but a lot of times they can abstract
[00:37:55] away the technical details, which is
[00:37:58] nice again for a demo, but then if it
[00:38:00] breaks, if it fails on you, then it's
[00:38:03] like uh like you have to end up going in
[00:38:05] looking at the code and trying to do it.
[00:38:07] So yes, you know, you can use them for
[00:38:10] demos, but I tell a lot of people for
[00:38:12] many times as you move these to
[00:38:14] productions, it's best to not really
[00:38:16] rely on those agentic frameworks unless
[00:38:18] you have to um to do that because it's a
[00:38:21] dependency that the world is changing so
[00:38:23] fast. I haven't seen any of them stay up
[00:38:26] in a reliable way versus just going with
[00:38:28] straight Python.
[00:38:30] >> [snorts]
[00:38:30] >> It's a similar thing that we see with
[00:38:32] with agentic pieces where you can build
[00:38:35] these workflows of I have certain
[00:38:37] actions that I want to do. You can
[00:38:40] orchestrate it yourself where you have
[00:38:42] control over every step like that. For
[00:38:45] some applications where you want
[00:38:46] control, you want to make sure
[00:38:48] everything works perfectly, it makes
[00:38:50] sense to do that. Now, the model
[00:38:53] providers are busy training their models
[00:38:55] to follow these types of workflows. And
[00:38:57] so they're saying, "Don't don't create
[00:38:59] these workflows. Don't create these
[00:39:01] separate steps. Instead, our agents are
[00:39:04] smart enough. We'll dynamically do it.
[00:39:06] You can just trust us to do that." And
[00:39:09] my guess is is for different
[00:39:10] applications, we'll do different things.
[00:39:12] But I'm just want you to be aware of
[00:39:14] sometimes you want to break everything
[00:39:15] down into specific pieces. Sometimes eh,
[00:39:19] let's just see if the LM can handle it
[00:39:20] by itself without anything else um like
[00:39:23] that.
[00:39:25] All right. Thank you all. I went through
[00:39:27] this quickly, but hopefully it gives
[00:39:29] everybody a sense of like where are the
[00:39:31] issues with Genai that you should be
[00:39:33] thinking of. Hopefully the confidence to
[00:39:36] be like, okay, I saw how Raj did this
[00:39:38] with this application. Let me try to
[00:39:41] build my own and this is what you should
[00:39:42] do, my own Genai application where I
[00:39:44] build a simple evaluation data. I can
[00:39:47] test whether my model is meeting it or
[00:39:49] not like that. And just do that to give
[00:39:51] yourself the experience and the
[00:39:53] confidence to do this kind of work. All
[00:39:55] right. Thank you all.